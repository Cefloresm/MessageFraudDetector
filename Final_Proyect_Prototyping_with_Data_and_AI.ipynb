{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOYRTsetvGp5WdFxqDFphcL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cefloresm/MessageFraudDetector/blob/master/Final_Proyect_Prototyping_with_Data_and_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps to make our fraud detector\n",
        "\n",
        "\n",
        "1 Transform our data - Libraries: **Pycaret** or **NLTK**\n",
        "\n",
        "**MUST DOs in Text Mining techniques!**\n",
        "\n",
        "- **Tokenization**: Split into sentences >  sentences into words > Transform everything to lower case > remove punctuations\n",
        "- **Remove all stopwords** (there are dictionaries to help us do this)\n",
        "- **Lemmatize your words** (for ex. Changing from 3rd person into 1st person, changing verbs from past/future tenses into present tenses)\n",
        "- **Stem your words** (for example “walking” and “walked” are reduced to “walk”)\n",
        "\n",
        "\n",
        "2 Count vectorizer - Conteo sencillo de cuantas palabras hay\n",
        "Ejemplo:\n",
        "text_list= []\n",
        "vectorizer= countvectorizer()\n",
        "vectorizer.fit (text_list_\n",
        "x= vectorizer.transform (list2)\n",
        "x= [] -> Un dataframe con todas las palabras en columnas y un conteo de cuanto\n",
        "\n",
        "\n",
        "3 Balance the data (Fraud vs Non fraud)\n",
        "\n",
        "\n",
        "4 Plug el dataset limpio al modelo de ML.\n",
        "\n",
        "Intentar primero con Supervised learning con **Random Forests** o con **Logistic Regression.**\n",
        "\n",
        "O TAMBIEN puede ser\n",
        "\n",
        "Unsupervised learning con el algoritmo **\"LDA model\"** parecido a Kmeans que divide la data en diferentes \"clusters\" o segmentos.\n",
        "\n",
        "5.Con la probabilidad del modelo y el output, conectarlo con gmail y enviar al usuario un aviso de **BAJA** probabilidad de fraude o **ALTA** probabilidad de fraude.\n"
      ],
      "metadata": {
        "id": "phVdgrDJll9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BACK END"
      ],
      "metadata": {
        "id": "WvaVwcYrawEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0 Data cleaning\n"
      ],
      "metadata": {
        "id": "wJxeff0feIir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBplsjRIlOCh"
      },
      "outputs": [],
      "source": [
        "# Load pandas package to read tables (dataframes)\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file with proper delimiter\n",
        "data = pd.read_csv('fraud_call.csv')\n",
        "\n",
        "# Display the transformed data\n",
        "data.head()"
      ],
      "metadata": {
        "id": "nXZUNlvR9CjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the first column into multiple parts based on the observed delimiter\n",
        "split_data = data.iloc[:, 0].str.split(r'\\s+', expand=True, n=1)\n",
        "\n",
        "# Select only the first two columns and rename them\n",
        "cleaned_data = split_data.iloc[:, :2]\n",
        "cleaned_data.columns = ['Fraud/normal', 'Message']\n",
        "\n",
        "## Handle any missing values by filling with an appropriate placeholder\n",
        "cleaned_data.loc[:, 'Message'] = cleaned_data['Message'].fillna('')\n",
        "\n",
        "# Display the cleaned data\n",
        "print(cleaned_data.head(100))\n",
        "cleaned_data= df= pd.read_csv('cleaned_merged_data.csv') #Lets now work with the combined dataset that I merged 20/06/2024\n",
        "print(cleaned_data.head)"
      ],
      "metadata": {
        "id": "WkPUQDzONlRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_data.dtypes)\n"
      ],
      "metadata": {
        "id": "8jzpqyjOR6AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data.describe()"
      ],
      "metadata": {
        "id": "i7AgoFz9eYNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find non-unique messages\n",
        "non_unique_messages = cleaned_data[cleaned_data.duplicated(subset=['Message'], keep=False)]\n",
        "print (non_unique_messages)\n",
        "\n",
        "# Find unique messages\n",
        "unique_messages = cleaned_data[~cleaned_data.duplicated(subset=['Message'], keep=False)]\n",
        "\n",
        "# Making the main dataframe as 'df' making it equal with the cleaned dataset (unique_messages)\n",
        "df= unique_messages\n",
        "print(df)\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "id": "bQ4wwPTbfeNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count of normal and fraud\n",
        "pncount = cleaned_data['Fraud/normal'].value_counts()\n",
        "print(pncount)\n",
        "\n",
        "# % of positive (fraud) and negative (nonfraud) class in data\n",
        "pnpercentage= pncount/len(cleaned_data)\n",
        "print(pnpercentage*100)"
      ],
      "metadata": {
        "id": "BJDWSs9Pf02m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing text data with Pycaret"
      ],
      "metadata": {
        "id": "ICl2N0UUyR_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycaret[nlp]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "G8w9bWCcznFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk gensim pyLDAvis"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NP0Ur1veDgMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 0.5: Tokenization"
      ],
      "metadata": {
        "id": "gTrRRo5nJV4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import required libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NM3ixjba71fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar stopwords y tokenizer de NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CGnAIcvi8d6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove punctuations and convert to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply the clean_text function to each message and replace the original column\n",
        "df['Message'] = df['Message'].apply(clean_text)\n",
        "\n",
        "# Display the DataFrame with cleaned messages\n",
        "print(df)"
      ],
      "metadata": {
        "id": "JcO1JBzY9rW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Count Vectorization of Tokenized Text\n"
      ],
      "metadata": {
        "id": "6QvFOdpdJb-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "ILG-DIrVHj6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words='english')\n",
        "dtm = cv.fit_transform(df['Message'])\n",
        "\n",
        "# Convert the document-term matrix to a DataFrame for better visualization\n",
        "dtm_df = pd.DataFrame(dtm.toarray(), columns=cv.get_feature_names_out())\n",
        "\n",
        "# Select the row you are interested in (e.g., first row)\n",
        "row_index = 0\n",
        "row_data = dtm_df.iloc[row_index]\n",
        "\n",
        "# Filter the row to show only columns with a value greater than 0\n",
        "non_zero_columns = row_data[row_data > 0]\n",
        "\n",
        "# Display the non-zero columns\n",
        "print(f\"Non-zero columns for row {row_index}:\")\n",
        "print(non_zero_columns)\n",
        "\n",
        "print(dtm_df.columns)"
      ],
      "metadata": {
        "id": "tp_LuEMQKp_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: LDA Model, Fit + Transform Document/Term Matrix (dtm)"
      ],
      "metadata": {
        "id": "L4vh_6s0JcgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "metadata": {
        "id": "2ZmNzK7EH1AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LDA Model with GridSearch parameters\n",
        "lda_model = LatentDirichletAllocation(n_components=8,\n",
        "                                      learning_decay=0.5,\n",
        "                                      max_iter=50,\n",
        "                                      learning_method='online',\n",
        "                                      random_state=42,\n",
        "                                      batch_size=5000,\n",
        "                                      evaluate_every = -1,\n",
        "                                      n_jobs = -1)\n",
        "\n",
        "lda_output = lda_model.fit_transform(dtm)"
      ],
      "metadata": {
        "id": "XpBZvEc0OuJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Manual Review of Top Topic Features for Each Topic"
      ],
      "metadata": {
        "id": "1OmIvOyXJdxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index,topic in enumerate(lda_model.components_):\n",
        "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
        "    print([cv.get_feature_names_out()[i] for i in topic.argsort()[-15:]])\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "ois96mtAH1Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: pyLDAvis-Interactive Visualization of LDA Model Output"
      ],
      "metadata": {
        "id": "jeKiioznJ30q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "0UsVTwJTSmwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvispyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "pjyREvEIRtc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.lda_model\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.lda_model.prepare(lda_model, dtm, cv, mds='tsne')\n",
        "panel"
      ],
      "metadata": {
        "id": "aFdb2xt1J6HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Attribute Labels to Topics"
      ],
      "metadata": {
        "id": "htts7bn6J6iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "test_df = df[df['Fraud/normal'] == 'fraud']\n",
        "fraud_messages = test_df['Message'].tolist()\n",
        "\n",
        "# Transform the new data using the same CountVectorizer\n",
        "new_data_dtm = cv.transform(fraud_messages)\n",
        "\n",
        "# Use the trained LDA model to predict the topic distribution\n",
        "topic_distribution = lda_model.transform(new_data_dtm)\n",
        "\n",
        "# Initialize a counter for topics\n",
        "topic_counts = {i: 0 for i in range(lda_model.n_components)}\n",
        "\n",
        "# Display the topic distribution and the topic with the highest weight for each new document\n",
        "for i, dist in enumerate(topic_distribution):\n",
        "    max_topic = np.argmax(dist)\n",
        "    topic_counts[max_topic] += 1\n",
        "\n",
        "# Display the counts of each topic\n",
        "print(\"\\nCounts of each topic being the highest:\")\n",
        "for topic, count in topic_counts.items():\n",
        "    print(f\"Topic {topic}: {count} documents\")"
      ],
      "metadata": {
        "id": "lueZOl8lUeUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows that topic #6,0 (7 and 1 in the graph) are the 2 highest with frauds.  "
      ],
      "metadata": {
        "id": "8g7d6jUDi1Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Supervised Learning approach\n",
        "\n",
        "Let's try using the tokenized texted with the fraud-non fraud classification column to see how a supervised learning model works."
      ],
      "metadata": {
        "id": "Gbc_dPcDlIg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets use the dtm_df dataframe that is already tokenized\n",
        "dtm_df\n",
        "\n",
        "# Combine with the original 'Fraud/normal' column\n",
        "df = pd.concat([df['Fraud/normal'].reset_index(drop=True), dtm_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Print the combined DataFrame to verify\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "vcN1JqHElUm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycaret.classification import *"
      ],
      "metadata": {
        "id": "4ktz1yjWqGsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pycaret.classification import setup\n",
        "\n",
        "# Setup PyCaret\n",
        "setup(data=df, target= 'Fraud/normal', session_id=1, train_size= 0.8)"
      ],
      "metadata": {
        "id": "-7sXFFb6q0L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr= create_model('lr')"
      ],
      "metadata": {
        "id": "gxKirdYrt_e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(lr)"
      ],
      "metadata": {
        "id": "RtMuWb1vt0LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Testing working Supervised Learning model (Random Forest) with a real example:"
      ],
      "metadata": {
        "id": "jYGlRIC7Af4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample message\n",
        "sample_message = {'Message': ['Your delivery has been suspended due to a lack of a street no. Please update.']}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "sample_df = pd.DataFrame(sample_message)\n",
        "\n",
        "# Clean and tokenize the message (assume you have a clean_text function)\n",
        "sample_df['Message'] = sample_df['Message'].apply(clean_text)\n",
        "\n",
        "# Transform the message using the same CountVectorizer\n",
        "transformed_message = cv.transform(sample_df['Message'])\n",
        "\n",
        "# Convert the transformed message to a DataFrame\n",
        "transformed_df = pd.DataFrame(transformed_message.toarray(), columns=cv.get_feature_names_out())"
      ],
      "metadata": {
        "id": "L2LZGQrcX0z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_model(lr, data= transformed_df)"
      ],
      "metadata": {
        "id": "CHpbGD8BUwda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('hello')"
      ],
      "metadata": {
        "id": "F-jrJFXXib4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Put it all inside a function called \"Fraud detection\""
      ],
      "metadata": {
        "id": "_kwIPL3zWT8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def fraud_detector(Recieved_msg=None):\n",
        "  email_message= 'You are a £1000 winner or Guaranteed Caller Prize, this is our Final attempt to contact you! To Claim Call 09071517866 Now! 150ppmPOBox10183BhamB64XE'\n",
        "\n",
        "  # Descargar stopwords y tokenizer de NLTK\n",
        "  nltk.download('stopwords')\n",
        "  nltk.download('punkt')\n",
        "\n",
        "  # Clean the email message\n",
        "  cleaned_message = clean_text(email_message)\n",
        "  print(cleaned_message)\n",
        "  # Create a DataFrame from the cleaned message\n",
        "  data = {'Message': [cleaned_message]}\n",
        "  df = pd.DataFrame(data)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Call the fraud_detector function and print the DataFrame\n",
        "#df = fraud_detector()\n",
        "#print(df)\n",
        "\n",
        "\n",
        "  #cv = CountVectorizer(stop_words='english')\n",
        "  #cvtest = cv.fit_transform(df['Message'])\n",
        "\n",
        "  # Convert the document-term matrix to a DataFrame for better visualization\n",
        "  #cvtest_df = pd.DataFrame(dtm.toarray(), columns=cv.get_feature_names_out())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DfI9vzdBdrfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FRONT END\n",
        "\n",
        "Recieve email at x@gmail.com\n",
        "\n",
        "Transform it and run the model\n",
        "\n",
        "Generate message: <<CAUTION!!! xyz@gmail.com. There is a x% chance that this message is fraud. Please do not\n",
        "\n",
        "            f\"⚠️ ALERT: This email has a high probability ({probability:.2f}%) of being fraudulent.\\n\"\n",
        "            \"Suggested Action:\\n\"\n",
        "            \"1. Do not click on any links or download any attachments in the email.\\n\"\n",
        "            \"2. Do not provide any personal information or financial details.\\n\"\n",
        "            \"3. Report this email to your IT/security department immediately.\\n\"\n",
        "            \"4. Delete the email from your inbox.\\n\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "V0s2TK60-MhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try with Streamlit\n"
      ],
      "metadata": {
        "id": "uZ9ogMD9jGr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "id": "i3OF0iiTj-hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Gmail API to read/send emails"
      ],
      "metadata": {
        "id": "uQzQLjqZ6K7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install simplegmail"
      ],
      "metadata": {
        "id": "qyZLoOiD6OhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1- Recieve e-mail"
      ],
      "metadata": {
        "id": "gzJz_Fs7dJdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simplegmail import Gmail\n",
        "\n",
        "gmail= Gmail()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LpUWBuQIdIw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2- Process it with created function"
      ],
      "metadata": {
        "id": "FqWf5HpmdKR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_detector(Recieved_msg=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "bUuTS3QDbd3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3- Replying to sender"
      ],
      "metadata": {
        "id": "7ZxCFLEpdT42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from email.message import EmailMessage\n",
        "import ssl\n",
        "import smtplib\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_bible_verse():\n",
        "    # Placeholder function for getting the Bible verse\n",
        "    return \"John 3:16 - For God so loved the world...\"\n",
        "\n",
        "# Login and sending email (Sender and recipient)\n",
        "def send_email():\n",
        "    print('Preparing to send e-mail...')\n",
        "    email_sender = userdata.get('gmail_CE')\n",
        "    email_password = userdata.get('gmailpass_CE')\n",
        "    email_receiver = \"galapito100@gmail.com\"\n",
        "\n",
        "    subject = \"ALERT: This email has a high probability of being fraudulent.\"\n",
        "\n",
        "    body= f\"\"\"\n",
        "    Suggested Action:\n",
        "        1. Do not click on any links or download any attachments in the email.\n",
        "        2. Do not provide any personal information or financial details.\n",
        "        3. Report this email to your IT/security department immediately.\n",
        "        4. Delete the email from your inbox.\n",
        "    \"\"\"\n",
        "\n",
        "    em = EmailMessage()\n",
        "    em['From'] = email_sender\n",
        "    em['To'] = email_receiver\n",
        "    em['Subject'] = subject\n",
        "    em.set_content(body)\n",
        "\n",
        "    context = ssl.create_default_context()\n",
        "\n",
        "    with smtplib.SMTP_SSL('smtp.gmail.com', 465, context=context) as smtp:\n",
        "        smtp.login(email_sender, email_password)\n",
        "        smtp.sendmail(email_sender, email_receiver, em.as_string())\n",
        "\n",
        "    print('Email sent')\n",
        "\n",
        "send_email()\n"
      ],
      "metadata": {
        "id": "XKZeBNsKeVzl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}